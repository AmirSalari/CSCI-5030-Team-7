{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import random\n","from tqdm import tqdm\n","import tensorflow.keras\n","import tensorflow as tf\n","from sklearn.metrics import fbeta_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.layers import Embedding,LSTM, TimeDistributed, Dense, Bidirectional\n","from tensorflow.keras.initializers import HeNormal, GlorotNormal, GlorotUniform\n","from nltk.translate.bleu_score import sentence_bleu\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('preprocessed_15.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.options.display.max_colwidth = 500\n","data[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess(t, add_start_token, add_end_token):\n","\n","  if add_start_token == True and add_end_token == False:\n","    t = '<start>'+' '+t\n","  if add_start_token == False and add_end_token == True:\n","    t = t+' '+'<end>'\n","  if add_start_token == True and add_end_token == True:\n","    t = '<start>'+' '+t+' '+'<end>'\n","\n","  t = re.sub(' +', ' ', t)\n","  return t"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoder_input = [preprocess(line, add_start_token= True, add_end_token=True) for line in data['error']]\n","decoder_input = [preprocess(line, add_start_token= True, add_end_token=False) for line in data['correct']]\n","decoder_output = [preprocess(line, add_start_token= False, add_end_token=True) for line in data['correct']]\n","print(encoder_input[0])\n","print(decoder_input[0])\n","print(decoder_output[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#ENCODER INPUT\n","\n","tokenizer = Tokenizer(filters='', split=\" \")\n","tokenizer.fit_on_texts(encoder_input)\n","word_index = tokenizer.word_index #vocabulary\n","\n","max_length = max([ len(row.split(\" \")) for row in encoder_input ])\n","INPUT_ENCODER_LENGTH = max_length\n","\n","enc_input_encoded = tokenizer.texts_to_sequences(encoder_input)\n","enc_input_padded= pad_sequences(enc_input_encoded, maxlen=INPUT_ENCODER_LENGTH, padding=\"post\")\n","\n","print(enc_input_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(encoder_input[0])\n","print(enc_input_padded[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#DECODER INPUT\n","decoder_data = decoder_input.copy()\n","decoder_data.extend(decoder_output)\n","\n","out_tokenizer = Tokenizer(filters='', split=\" \")\n","out_tokenizer.fit_on_texts(decoder_data)\n","word_index = out_tokenizer.word_index #vocabulary\n","\n","max_length = max([ len(row.split(\" \")) for row in decoder_input ])\n","INPUT_DECODER_LENGTH = max_length"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dec_input_encoded = out_tokenizer.texts_to_sequences(decoder_input)\n","dec_input_padded= pad_sequences(dec_input_encoded, maxlen=INPUT_DECODER_LENGTH, padding=\"post\", truncating = \"post\")\n","\n","print(dec_input_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(decoder_input[0])\n","print(dec_input_padded[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dec_output_encoded = out_tokenizer.texts_to_sequences(decoder_output)\n","dec_output_padded= pad_sequences(dec_output_encoded, maxlen=INPUT_DECODER_LENGTH, padding=\"post\", truncating = \"post\")\n","\n","print(dec_output_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(decoder_output[1])\n","print(dec_output_padded[1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Reference: https://fasttext.cc/docs/en/english-vectors.html\n","import io\n","\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        data[tokens[0]] = np.asarray(tokens[1:])#map(float, tokens[1:])\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_index = load_vectors('wiki-news-300d-1M.vec')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://keras.io/examples/nlp/pretrained_word_embeddings/\n","word_index = tokenizer.word_index\n","num_tokens = len(word_index) + 2\n","embedding_dim = 300\n","hits = 0\n","misses = 0\n","\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","\n","    if type(embedding_vector) == np.ndarray and embedding_vector.shape[0] == 300:  \n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))\n","np.save('GEC/in_embedding.npy', embedding_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_index = out_tokenizer.word_index\n","num_tokens = len(word_index) + 2\n","embedding_dim = 300\n","hits = 0\n","misses = 0\n","\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","\n","    if type(embedding_vector) == np.ndarray and embedding_vector.shape[0] == 300:  \n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))\n","np.save('GEC/out_embedding.npy', embedding_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["in_embedding_matrix = np.load('GEC/in_embedding.npy')\n","out_embedding_matrix = np.load('GEC/out_embedding.npy')\n","print(in_embedding_matrix.shape, out_embedding_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#ENCODER\n","class Encoder(tf.keras.Model):\n","    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n","        super().__init__()\n","        self.vocab_size = inp_vocab_size\n","        self.embedding_size = embedding_size\n","        self.lstm_units = lstm_size\n","        self.input_length = input_length\n","\n","def build(self, input_sequence):\n","        #self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length, \n","        #                           #embeddings_initializer=keras.initializers.Constant(in_embedding_matrix), mask_zero=True, \n","        #                           weights = [in_embedding_matrix], mask_zero=True, \n","        #                           trainable = False, name=\"embedding_layer_encoder\")\n","        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n","                           mask_zero=True, name=\"embedding_layer_encoder\")\n","        self.lstm = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n","\n","    def call(self,input_sequence,states, training = True):\n","        input_embedding = self.embedding(input_sequence)   #(batch_size, length of input array, embedding_size)\n","        self.lstm_output, self.state_h, self.state_c = self.lstm(input_embedding, initial_state = states)\n","        return self.lstm_output,self.state_h, self.state_c\n","\n","\n","    def initialize_states(self,batch_size):\n","      initializer = GlorotNormal()\n","      lstm_state_h = initializer(shape=(batch_size, self.lstm_units))#tf.zeros((batch_size, self.lstm_units), dtype=tf.dtypes.float32, name=\"Encoder_LSTM_hidden_state\")\n","      lstm_state_c = initializer(shape=(batch_size, self.lstm_units))#tf.zeros((batch_size, self.lstm_units), dtype=tf.dtypes.float32, name=\"Encoder_LSTM_cell_state\")\n","      return lstm_state_h, lstm_state_c\n","\n","\n","#DECODER\n","class Decoder(tf.keras.Model):\n","    def init(self,out_vocab_size,embedding_size,lstm_size,input_length):\n","        super().init()\n","        self.vocab_size = out_vocab_size\n","        self.embedding_size = embedding_size\n","        self.lstm_units = lstm_size\n","        self.input_length = input_length\n","\n","\n","    def build(self,input_sequence):\n","        #self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length, \n","        #                           #embeddings_initializer=keras.initializers.Constant(out_embedding_matrix), \n","        #                           weights = [out_embedding_matrix], mask_zero=True, \n","        #                           trainable = False, name=\"embedding_layer_decoder\")\n","        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n","                           mask_zero=True, name=\"embedding_layer_decoder\") \n","        self.lstm = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n","\n","\n","    def call(self,input_sequence,initial_states, training = True):\n","\n","        input_embedding = self.embedding(input_sequence)\n","        self.lstm_output, self.state_h, self.state_c = self.lstm(input_embedding, initial_state=initial_states)\n","        return self.lstm_output,self.state_h, self.state_c"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Encoder_decoder(tf.keras.Model):\n","    \n","    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n","\n","        super().__init__()\n","        self.encoder = Encoder(INPUT_VOCAB_SIZE, embedding_size = 256, lstm_size= 1200 , input_length= INPUT_ENCODER_LENGTH)\n","        self.decoder = Decoder(OUTPUT_VOCAB_SIZE, embedding_size = 256, lstm_size = 1200, input_length = None)\n","        self.dense = Dense(output_vocab_size)#, activation = 'softmax')\n","    \n","    def call(self,data):\n","        input, output = data[0], data[1]\n","        states = self.encoder.initialize_states(input.shape[0])\n","        encoder_output,encoder_final_state_h,encoder_final_state_c = self.encoder(input, states)\n","        decoder_output,decoder_state_h,decoder_state_c = self.decoder(output,[encoder_final_state_h,encoder_final_state_c])\n","        outputs = self.dense(decoder_output)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["INPUT_VOCAB_SIZE = len(list(tokenizer.word_index)) +1 #for zero padding +OOV\n","OUTPUT_VOCAB_SIZE = len(list(out_tokenizer.word_index)) +1 #for zero padding + OOV\n","BATCH_SIZE = 16\n","print(INPUT_VOCAB_SIZE, INPUT_ENCODER_LENGTH, OUTPUT_VOCAB_SIZE, INPUT_DECODER_LENGTH, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataset = full_dataset.take(50).batch(32)\n","train_dataset = full_dataset.skip(50).batch(32)\n","\n","print(train_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#LEARNING RATE SCHEDULER: Decay learning rate after 15 epochs\n","def scheduler(epoch, lr):\n","   if epoch < 1:\n","     return lr   \n","   else:\n","     return lr * tf.math.exp(-0.1)\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","\n","#EARLY STOPPING\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n","\n","#TENSORBOARD PLOTS\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir='logs')\n","\n","#SAVE MODEL WEIGHTS\n","class SaveModel(tf.keras.callbacks.Callback):\n","\n","  def __init__(self):\n","    self.history = { 'loss' : [],  'val_loss' : []}\n","    self.init = 0\n","\n","  def on_epoch_end(self, epoch, logs = {}):\n","    \n","    self.history['loss'].append(logs.get('loss'))\n","    if logs.get('val_loss', -1) != -1:\n","        self.history['val_loss'].append(logs.get('val_loss'))\n","\n","    #if epochs % 10 == 0:\n","    self.model.save_weights('GEC/ENC_DEC_EMB/weights_{}.h5'.format(epoch+self.init))    #print('Saved weights for epoch {}!'.format(epoch))\n","\n","    df = pd.DataFrame(columns = ['loss','val_loss']) \n","    for col in df.columns:\n","      df[col] = self.history[col]\n","    df.to_csv('history.csv')\n","    !cp history.csv \"GEC//ENC_DEC_EMB/history.csv\"\n","\n","save_model = SaveModel()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://www.tensorflow.org/tutorials/text/image_captioning#model\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none'\n",")\n","\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def f_beta_score(y_true, y_pred):\n","  y_pred_sparse = tf.convert_to_tensor(np.argmax(y_pred, axis = -1), dtype = tf.float32)\n","  fb_score = [ fbeta_score(y_true[i], y_pred_sparse[i],average = 'macro',beta = 0.5) for i in range(y_true.shape[0])]#tf.py_function(fbeta_score, inp = [y, y_pred, 0.5], Tout=tf.float32)\n","  return sum(fb_score)/len(fb_score)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"GEC_Baseline_Encoder_Decoder.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
