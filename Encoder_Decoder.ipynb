{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import random\n","from tqdm import tqdm\n","import tensorflow.keras\n","import tensorflow as tf\n","from sklearn.metrics import fbeta_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.layers import Embedding,LSTM, TimeDistributed, Dense, Bidirectional\n","from tensorflow.keras.initializers import HeNormal, GlorotNormal, GlorotUniform\n","from nltk.translate.bleu_score import sentence_bleu\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('preprocessed_15.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.options.display.max_colwidth = 500\n","data[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess(t, add_start_token, add_end_token):\n","\n","  if add_start_token == True and add_end_token == False:\n","    t = '<start>'+' '+t\n","  if add_start_token == False and add_end_token == True:\n","    t = t+' '+'<end>'\n","  if add_start_token == True and add_end_token == True:\n","    t = '<start>'+' '+t+' '+'<end>'\n","\n","  t = re.sub(' +', ' ', t)\n","  return t"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoder_input = [preprocess(line, add_start_token= True, add_end_token=True) for line in data['error']]\n","decoder_input = [preprocess(line, add_start_token= True, add_end_token=False) for line in data['correct']]\n","decoder_output = [preprocess(line, add_start_token= False, add_end_token=True) for line in data['correct']]\n","print(encoder_input[0])\n","print(decoder_input[0])\n","print(decoder_output[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#ENCODER INPUT\n","\n","tokenizer = Tokenizer(filters='', split=\" \")\n","tokenizer.fit_on_texts(encoder_input)\n","word_index = tokenizer.word_index #vocabulary\n","\n","max_length = max([ len(row.split(\" \")) for row in encoder_input ])\n","INPUT_ENCODER_LENGTH = max_length\n","\n","enc_input_encoded = tokenizer.texts_to_sequences(encoder_input)\n","enc_input_padded= pad_sequences(enc_input_encoded, maxlen=INPUT_ENCODER_LENGTH, padding=\"post\")\n","\n","print(enc_input_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(encoder_input[0])\n","print(enc_input_padded[0])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"GEC_Baseline_Encoder_Decoder.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
